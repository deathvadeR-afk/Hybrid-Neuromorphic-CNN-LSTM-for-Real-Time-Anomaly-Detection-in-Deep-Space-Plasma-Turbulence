{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13006210,"sourceType":"datasetVersion","datasetId":8233375}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-10T04:09:46.846895Z","iopub.execute_input":"2025-09-10T04:09:46.847143Z","iopub.status.idle":"2025-09-10T04:09:47.163212Z","shell.execute_reply.started":"2025-09-10T04:09:46.847123Z","shell.execute_reply":"2025-09-10T04:09:47.162627Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cnn-lstm-training-output/cnn_training_output/cnn_training_output/confusion_matrix.png\n/kaggle/input/cnn-lstm-training-output/cnn_training_output/cnn_training_output/best_plasma_cnn_model.pth\n/kaggle/input/cnn-lstm-training-output/cnn_training_output/cnn_training_output/training_history.png\n/kaggle/input/cnn-lstm-training-output/cnn_training_output/cnn_training_output/roc_curve.png\n/kaggle/input/cnn-lstm-training-output/snn_training_output/snn_training_output/snn_training_results.png\n/kaggle/input/cnn-lstm-training-output/snn_training_output/snn_training_output/best_plasma_snn_model.pth\n/kaggle/input/cnn-lstm-training-output/snn_training_output/snn_training_output/snn_training_metrics.pth\n/kaggle/input/cnn-lstm-training-output/snn_training_output/snn_training_output/snn_training_metrics.json\n/kaggle/input/cnn-lstm-training-output/lstm_training_output/lstm_training_output/lstm_training_results.png\n/kaggle/input/cnn-lstm-training-output/lstm_training_output/lstm_training_output/best_plasma_lstm_model.pth\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install snntorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T06:56:47.388011Z","iopub.execute_input":"2025-09-10T06:56:47.388695Z","iopub.status.idle":"2025-09-10T06:56:51.458979Z","shell.execute_reply.started":"2025-09-10T06:56:47.388673Z","shell.execute_reply":"2025-09-10T06:56:51.458139Z"}},"outputs":[{"name":"stdout","text":"Collecting snntorch\n  Downloading snntorch-0.9.4-py2.py3-none-any.whl.metadata (15 kB)\nDownloading snntorch-0.9.4-py2.py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: snntorch\nSuccessfully installed snntorch-0.9.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nWorking SNN Architecture - Based on Progressive Testing Success\nUses Level 2 architecture (no temporal processing) for reliable performance\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport snntorch as snn\nfrom snntorch import surrogate\nimport numpy as np\nfrom sklearn.metrics import classification_report, balanced_accuracy_score\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport time\nimport json\nfrom pathlib import Path\nimport warnings\nimport random\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸ”§ Working SNN Architecture on: {device}\")\n\nclass PlasmaLikeDataset(Dataset):\n    \"\"\"More realistic dataset simulating plasma features\"\"\"\n    def __init__(self, num_samples=2000, input_size=512, anomaly_ratio=0.3):\n        self.num_samples = num_samples\n        self.input_size = input_size\n        self.anomaly_ratio = anomaly_ratio\n        \n        print(f\"ğŸ”„ Creating plasma-like dataset: {num_samples} samples, {anomaly_ratio:.1%} anomalies\")\n        self.data, self.labels = self._generate_plasma_like_data()\n        \n        # Verify class balance\n        unique, counts = torch.unique(self.labels, return_counts=True)\n        print(f\"   Class distribution: {dict(zip(unique.tolist(), counts.tolist()))}\")\n        \n    def _generate_plasma_like_data(self):\n        \"\"\"Generate data that simulates plasma turbulence features\"\"\"\n        num_anomalies = int(self.num_samples * self.anomaly_ratio)\n        num_normal = self.num_samples - num_anomalies\n        \n        # Normal plasma patterns (based on 13 plasma features from memory)\n        normal_data = []\n        for i in range(num_normal):\n            # Simulate magnetic field components and derived features\n            base_pattern = torch.randn(self.input_size) * 0.5\n            \n            # Add plasma-like correlations\n            b_field_strength = torch.randn(1) * 0.3\n            base_pattern[:64] += b_field_strength  # Magnetic field components\n            base_pattern[64:128] += b_field_strength * 0.5  # Derived features\n            \n            normal_data.append(base_pattern)\n        \n        normal_data = torch.stack(normal_data)\n        normal_labels = torch.zeros(num_normal, dtype=torch.long)\n        \n        # Anomalous plasma patterns\n        anomaly_data = []\n        for i in range(num_anomalies):\n            base_pattern = torch.randn(self.input_size) * 0.5\n            \n            # Add clear anomaly signatures\n            anomaly_strength = 2.0 + np.random.random() * 1.0\n            \n            if i % 3 == 0:\n                # Magnetic field spike anomalies\n                base_pattern[:64] += torch.randn(64) * anomaly_strength\n            elif i % 3 == 1:\n                # Energy density anomalies\n                base_pattern[128:256] += torch.ones(128) * anomaly_strength\n            else:\n                # Turbulence spectrum anomalies\n                base_pattern[256:] += torch.sin(torch.linspace(0, 10, self.input_size-256)) * anomaly_strength\n            \n            anomaly_data.append(base_pattern)\n        \n        anomaly_data = torch.stack(anomaly_data)\n        anomaly_labels = torch.ones(num_anomalies, dtype=torch.long)\n        \n        # Combine and shuffle\n        data = torch.cat([normal_data, anomaly_data], dim=0)\n        labels = torch.cat([normal_labels, anomaly_labels], dim=0)\n        \n        indices = torch.randperm(len(data))\n        data = data[indices]\n        labels = labels[indices]\n        \n        print(f\"   Normal patterns: mean={normal_data.mean():.3f}, std={normal_data.std():.3f}\")\n        print(f\"   Anomaly patterns: mean={anomaly_data.mean():.3f}, std={anomaly_data.std():.3f}\")\n        \n        return data, labels\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n    \n    def get_balanced_sampler(self):\n        \"\"\"Get balanced sampler for training\"\"\"\n        class_counts = torch.bincount(self.labels)\n        class_weights = 1.0 / class_counts.float()\n        sample_weights = class_weights[self.labels]\n        return WeightedRandomSampler(sample_weights, len(sample_weights))\n\nclass WorkingSNN(nn.Module):\n    \"\"\"Working SNN based on successful Level 2 architecture\"\"\"\n    def __init__(self, input_size=512, hidden_size=256, output_size=2):\n        super(WorkingSNN, self).__init__()\n        \n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        \n        # Multi-layer architecture without temporal processing\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n        \n        # LIF neurons with conservative parameters\n        self.lif1 = snn.Leaky(beta=0.8, spike_grad=surrogate.straight_through_estimator())\n        self.lif2 = snn.Leaky(beta=0.8, spike_grad=surrogate.straight_through_estimator())\n        self.lif3 = snn.Leaky(beta=0.8, spike_grad=surrogate.straight_through_estimator(), output=True)\n        \n        # Conservative initialization\n        for layer in [self.fc1, self.fc2, self.fc3]:\n            nn.init.normal_(layer.weight, mean=0.0, std=0.01)\n            nn.init.zeros_(layer.bias)\n        \n        # Light regularization\n        self.dropout = nn.Dropout(0.1)\n        \n        print(f\"âœ… WorkingSNN: {input_size}â†’{hidden_size}â†’{hidden_size//2}â†’{output_size}\")\n    \n    def forward(self, x):\n        \"\"\"Single-step forward pass (no temporal processing)\"\"\"\n        # Fresh membrane potentials for each forward pass\n        mem1 = self.lif1.init_leaky()\n        mem2 = self.lif2.init_leaky()\n        mem3 = self.lif3.init_leaky()\n        \n        # Forward through network\n        cur1 = self.fc1(x)\n        spk1, mem1 = self.lif1(cur1, mem1)\n        spk1 = self.dropout(spk1)\n        \n        cur2 = self.fc2(spk1)\n        spk2, mem2 = self.lif2(cur2, mem2)\n        spk2 = self.dropout(spk2)\n        \n        cur3 = self.fc3(spk2)\n        spk3, mem3 = self.lif3(cur3, mem3)\n        \n        # Use membrane potential for classification\n        return mem3\n\nclass WorkingSNNTrainer:\n    \"\"\"Trainer optimized for working SNN architecture\"\"\"\n    def __init__(self, model, device):\n        self.model = model.to(device)\n        self.device = device\n        \n        # Balanced loss function\n        self.criterion = nn.CrossEntropyLoss()\n        \n        # Conservative optimizer\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(), \n            lr=0.001, \n            weight_decay=1e-5\n        )\n        \n        self.scheduler = torch.optim.lr_scheduler.StepLR(\n            self.optimizer, step_size=5, gamma=0.8\n        )\n        \n    def train_epoch(self, dataloader):\n        \"\"\"Training epoch\"\"\"\n        self.model.train()\n        \n        total_loss = 0\n        correct = 0\n        total = 0\n        class_correct = [0, 0]\n        class_total = [0, 0]\n        \n        for batch_idx, (data, labels) in enumerate(dataloader):\n            data, labels = data.to(self.device), labels.to(self.device)\n            \n            self.optimizer.zero_grad()\n            outputs = self.model(data)\n            loss = self.criterion(outputs, labels)\n            loss.backward()\n            \n            # Conservative gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            \n            # Metrics\n            total_loss += loss.item()\n            predicted = outputs.argmax(dim=1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            for i in range(2):\n                class_mask = labels == i\n                if class_mask.sum() > 0:\n                    class_correct[i] += ((predicted == labels) & class_mask).sum().item()\n                    class_total[i] += class_mask.sum().item()\n            \n            if batch_idx % 20 == 0:\n                class_acc_0 = class_correct[0] / max(class_total[0], 1) * 100\n                class_acc_1 = class_correct[1] / max(class_total[1], 1) * 100\n                balanced_acc = (class_acc_0 + class_acc_1) / 2\n                \n                print(f'  Batch {batch_idx}: Loss: {loss.item():.4f}, '\n                      f'Balanced: {balanced_acc:.1f}% (N:{class_acc_0:.1f}%, A:{class_acc_1:.1f}%)')\n        \n        return total_loss / len(dataloader), 100. * correct / total, class_correct, class_total\n    \n    def validate(self, dataloader):\n        \"\"\"Validation\"\"\"\n        self.model.eval()\n        \n        total_loss = 0\n        correct = 0\n        total = 0\n        class_correct = [0, 0]\n        class_total = [0, 0]\n        \n        all_predictions = []\n        all_labels = []\n        \n        with torch.no_grad():\n            for data, labels in dataloader:\n                data, labels = data.to(self.device), labels.to(self.device)\n                \n                outputs = self.model(data)\n                loss = self.criterion(outputs, labels)\n                \n                total_loss += loss.item()\n                predicted = outputs.argmax(dim=1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n                \n                all_predictions.extend(predicted.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                \n                for i in range(2):\n                    class_mask = labels == i\n                    if class_mask.sum() > 0:\n                        class_correct[i] += ((predicted == labels) & class_mask).sum().item()\n                        class_total[i] += class_mask.sum().item()\n        \n        return total_loss / len(dataloader), 100. * correct / total, class_correct, class_total, all_predictions, all_labels\n\ndef main():\n    \"\"\"Working SNN training for plasma anomaly detection\"\"\"\n    print(\"ğŸš€ WORKING SNN FOR PLASMA ANOMALY DETECTION\")\n    print(\"=\" * 60)\n    print(\"ğŸ§  Based on progressive testing: Level 2 architecture\")\n    print(\"ğŸ¯ Target: Stable balanced performance for plasma data\")\n    print(\"=\" * 60)\n    \n    output_dir = Path(\"working_snn_output\")\n    output_dir.mkdir(exist_ok=True)\n    \n    # Create plasma-like datasets\n    print(\"\\nğŸ“Š Creating plasma-like datasets...\")\n    train_dataset = PlasmaLikeDataset(num_samples=4000, input_size=512, anomaly_ratio=0.3)\n    val_dataset = PlasmaLikeDataset(num_samples=1000, input_size=512, anomaly_ratio=0.3)\n    test_dataset = PlasmaLikeDataset(num_samples=800, input_size=512, anomaly_ratio=0.3)\n    \n    # Balanced sampling\n    train_sampler = train_dataset.get_balanced_sampler()\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler, num_workers=0)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n    \n    # Initialize working SNN\n    print(\"\\nğŸ”§ Initializing working SNN...\")\n    model = WorkingSNN(input_size=512, hidden_size=256, output_size=2)\n    trainer = WorkingSNNTrainer(model, device)\n    \n    # Training loop\n    num_epochs = 15\n    best_balanced_acc = 0\n    patience = 5\n    patience_counter = 0\n    \n    print(f\"\\nğŸ§  Training working SNN for {num_epochs} epochs...\")\n    \n    for epoch in range(num_epochs):\n        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n        print(\"-\" * 40)\n        \n        # Training\n        train_loss, train_acc, train_class_correct, train_class_total = trainer.train_epoch(train_loader)\n        \n        # Validation\n        val_loss, val_acc, val_class_correct, val_class_total, val_preds, val_labels = trainer.validate(val_loader)\n        \n        # Calculate metrics\n        train_normal_acc = train_class_correct[0] / max(train_class_total[0], 1) * 100\n        train_anomaly_acc = train_class_correct[1] / max(train_class_total[1], 1) * 100\n        train_balanced_acc = (train_normal_acc + train_anomaly_acc) / 2\n        \n        val_normal_acc = val_class_correct[0] / max(val_class_total[0], 1) * 100\n        val_anomaly_acc = val_class_correct[1] / max(val_class_total[1], 1) * 100\n        val_balanced_acc = (val_normal_acc + val_anomaly_acc) / 2\n        \n        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.1f}%, Balanced: {train_balanced_acc:.1f}%\")\n        print(f\"       Normal: {train_normal_acc:.1f}%, Anomaly: {train_anomaly_acc:.1f}%\")\n        print(f\"Val   - Loss: {val_loss:.4f}, Acc: {val_acc:.1f}%, Balanced: {val_balanced_acc:.1f}%\")\n        print(f\"       Normal: {val_normal_acc:.1f}%, Anomaly: {val_anomaly_acc:.1f}%\")\n        \n        # Learning rate scheduling\n        trainer.scheduler.step()\n        \n        # Early stopping\n        if val_balanced_acc > best_balanced_acc:\n            best_balanced_acc = val_balanced_acc\n            patience_counter = 0\n            torch.save(model.state_dict(), output_dir / \"working_snn_model.pth\")\n            print(f\"âœ… New best model saved! Balanced Acc: {val_balanced_acc:.1f}%\")\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= patience:\n            print(f\"â° Early stopping after {patience} epochs without improvement\")\n            break\n        \n        # Check for collapse\n        if val_normal_acc < 10 or val_anomaly_acc < 10:\n            print(f\"âš ï¸ Warning: Potential class collapse detected!\")\n    \n    # Final evaluation\n    print(f\"\\nğŸ“Š FINAL EVALUATION\")\n    print(\"=\" * 30)\n    \n    # Load best model\n    model.load_state_dict(torch.load(output_dir / \"working_snn_model.pth\", weights_only=True))\n    \n    # Test evaluation\n    test_loss, test_acc, test_class_correct, test_class_total, test_preds, test_labels = trainer.validate(test_loader)\n    \n    test_normal_acc = test_class_correct[0] / max(test_class_total[0], 1) * 100\n    test_anomaly_acc = test_class_correct[1] / max(test_class_total[1], 1) * 100\n    test_balanced_acc = (test_normal_acc + test_anomaly_acc) / 2\n    \n    print(f\"ğŸ‰ WORKING SNN TRAINING COMPLETE!\")\n    print(f\"ğŸ“Š Best Validation Balanced Accuracy: {best_balanced_acc:.1f}%\")\n    print(f\"ğŸ“Š Test Results:\")\n    print(f\"   Overall Accuracy: {test_acc:.1f}%\")\n    print(f\"   Balanced Accuracy: {test_balanced_acc:.1f}%\")\n    print(f\"   Normal Accuracy: {test_normal_acc:.1f}%\")\n    print(f\"   Anomaly Accuracy: {test_anomaly_acc:.1f}%\")\n    \n    # Classification report\n    print(f\"\\nğŸ“‹ Classification Report:\")\n    print(classification_report(test_labels, test_preds, target_names=['Normal', 'Anomaly']))\n    \n    # Success analysis\n    stability_check = (test_normal_acc > 30 and test_anomaly_acc > 30)\n    performance_check = (test_balanced_acc > 70)\n    \n    print(f\"\\nğŸ¯ WORKING SNN ANALYSIS:\")\n    print(f\"Stability: {'âœ… STABLE' if stability_check else 'âŒ UNSTABLE'} (Both classes >30%)\")\n    print(f\"Performance: {'âœ… GOOD' if performance_check else 'âš ï¸ MODERATE'} (Balanced >{70 if not performance_check else test_balanced_acc:.0f}%)\")\n    \n    if stability_check and performance_check:\n        print(\"ğŸ‰ SUCCESS: Working SNN ready for integration!\")\n        recommendation = \"Integrate working SNN into hybrid CNN-LSTM-SNN pipeline\"\n    elif stability_check:\n        print(\"âš ï¸ PARTIAL SUCCESS: Stable but needs optimization\")\n        recommendation = \"Optimize hyperparameters and architecture\"\n    else:\n        print(\"âŒ NEEDS WORK: Still showing instability\")\n        recommendation = \"Consider simpler architectures or different approaches\"\n    \n    # Save results\n    results = {\n        'best_val_balanced_accuracy': best_balanced_acc,\n        'test_accuracy': test_acc,\n        'test_balanced_accuracy': test_balanced_acc,\n        'test_normal_accuracy': test_normal_acc,\n        'test_anomaly_accuracy': test_anomaly_acc,\n        'stability_achieved': stability_check,\n        'performance_target_met': performance_check,\n        'recommendation': recommendation,\n        'ready_for_integration': stability_check and performance_check\n    }\n    \n    with open(output_dir / \"working_snn_results.json\", 'w') as f:\n        json.dump(results, f, indent=2)\n    \n    print(f\"\\nğŸ¯ RECOMMENDATION: {recommendation}\")\n    print(f\"ğŸ“ Results saved to: {output_dir}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    torch.manual_seed(42)\n    np.random.seed(42)\n    random.seed(42)\n    \n    try:\n        results = main()\n        print(\"\\nâœ… Working SNN training completed!\")\n    except Exception as e:\n        print(f\"\\nâŒ Error during working SNN training: {e}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T07:09:10.390859Z","iopub.execute_input":"2025-09-10T07:09:10.391531Z","iopub.status.idle":"2025-09-10T07:09:14.679603Z","shell.execute_reply.started":"2025-09-10T07:09:10.391507Z","shell.execute_reply":"2025-09-10T07:09:14.678877Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Working SNN Architecture on: cuda\nğŸš€ WORKING SNN FOR PLASMA ANOMALY DETECTION\n============================================================\nğŸ§  Based on progressive testing: Level 2 architecture\nğŸ¯ Target: Stable balanced performance for plasma data\n============================================================\n\nğŸ“Š Creating plasma-like datasets...\nğŸ”„ Creating plasma-like dataset: 4000 samples, 30.0% anomalies\n   Normal patterns: mean=-0.001, std=0.514\n   Anomaly patterns: mean=0.283, std=1.207\n   Class distribution: {0: 2800, 1: 1200}\nğŸ”„ Creating plasma-like dataset: 1000 samples, 30.0% anomalies\n   Normal patterns: mean=-0.007, std=0.511\n   Anomaly patterns: mean=0.288, std=1.211\n   Class distribution: {0: 700, 1: 300}\nğŸ”„ Creating plasma-like dataset: 800 samples, 30.0% anomalies\n   Normal patterns: mean=-0.002, std=0.513\n   Anomaly patterns: mean=0.276, std=1.205\n   Class distribution: {0: 560, 1: 240}\n\nğŸ”§ Initializing working SNN...\nâœ… WorkingSNN: 512â†’256â†’128â†’2\n\nğŸ§  Training working SNN for 15 epochs...\n\nEpoch 1/15\n----------------------------------------\n  Batch 0: Loss: 0.6931, Balanced: 50.0% (N:100.0%, A:0.0%)\n  Batch 20: Loss: 0.5174, Balanced: 67.4% (N:89.2%, A:45.6%)\n  Batch 40: Loss: 0.5872, Balanced: 74.3% (N:94.6%, A:54.1%)\n  Batch 60: Loss: 0.4686, Balanced: 77.3% (N:96.0%, A:58.6%)\n  Batch 80: Loss: 0.4180, Balanced: 79.2% (N:96.8%, A:61.6%)\n  Batch 100: Loss: 0.4200, Balanced: 80.2% (N:97.2%, A:63.2%)\n  Batch 120: Loss: 0.4028, Balanced: 80.7% (N:97.4%, A:63.9%)\nTrain - Loss: 0.4947, Acc: 81.0%, Balanced: 81.0%\n       Normal: 97.5%, Anomaly: 64.4%\nVal   - Loss: 0.5508, Acc: 89.5%, Balanced: 83.6%\n       Normal: 98.3%, Anomaly: 69.0%\nâœ… New best model saved! Balanced Acc: 83.6%\n\nEpoch 2/15\n----------------------------------------\n  Batch 0: Loss: 0.5441, Balanced: 71.4% (N:100.0%, A:42.9%)\n  Batch 20: Loss: 0.3601, Balanced: 84.1% (N:99.1%, A:69.1%)\n  Batch 40: Loss: 0.3918, Balanced: 85.4% (N:99.5%, A:71.2%)\n  Batch 60: Loss: 0.5771, Balanced: 86.0% (N:99.2%, A:72.8%)\n  Batch 80: Loss: 0.4288, Balanced: 85.6% (N:98.9%, A:72.4%)\n  Batch 100: Loss: 0.3439, Balanced: 85.6% (N:98.8%, A:72.5%)\n  Batch 120: Loss: 0.3687, Balanced: 85.4% (N:98.7%, A:72.1%)\nTrain - Loss: 0.4358, Acc: 85.5%, Balanced: 85.4%\n       Normal: 98.6%, Anomaly: 72.3%\nVal   - Loss: 0.5192, Acc: 88.6%, Balanced: 83.2%\n       Normal: 96.7%, Anomaly: 69.7%\n\nEpoch 3/15\n----------------------------------------\n  Batch 0: Loss: 0.4576, Balanced: 90.4% (N:94.1%, A:86.7%)\n  Batch 20: Loss: 0.3625, Balanced: 86.1% (N:97.0%, A:75.1%)\n  Batch 40: Loss: 0.4392, Balanced: 85.5% (N:97.3%, A:73.6%)\n  Batch 60: Loss: 0.4206, Balanced: 85.7% (N:97.0%, A:74.4%)\n  Batch 80: Loss: 0.5310, Balanced: 85.9% (N:96.5%, A:75.2%)\n  Batch 100: Loss: 0.5597, Balanced: 85.8% (N:96.7%, A:74.8%)\n  Batch 120: Loss: 0.4618, Balanced: 85.7% (N:96.6%, A:74.7%)\nTrain - Loss: 0.4110, Acc: 85.7%, Balanced: 85.7%\n       Normal: 96.6%, Anomaly: 74.9%\nVal   - Loss: 0.5356, Acc: 87.5%, Balanced: 83.3%\n       Normal: 93.9%, Anomaly: 72.7%\n\nEpoch 4/15\n----------------------------------------\n  Batch 0: Loss: 0.5110, Balanced: 85.3% (N:92.9%, A:77.8%)\n  Batch 20: Loss: 0.5173, Balanced: 88.0% (N:94.6%, A:81.4%)\n  Batch 40: Loss: 0.4182, Balanced: 86.0% (N:94.5%, A:77.5%)\n  Batch 60: Loss: 0.2392, Balanced: 85.7% (N:94.4%, A:77.0%)\n  Batch 80: Loss: 0.7134, Balanced: 86.1% (N:94.5%, A:77.7%)\n  Batch 100: Loss: 0.3369, Balanced: 86.1% (N:93.7%, A:78.5%)\n  Batch 120: Loss: 0.5363, Balanced: 86.0% (N:93.5%, A:78.5%)\nTrain - Loss: 0.4323, Acc: 85.8%, Balanced: 85.9%\n       Normal: 93.3%, Anomaly: 78.5%\nVal   - Loss: 0.5649, Acc: 83.7%, Balanced: 81.6%\n       Normal: 86.9%, Anomaly: 76.3%\n\nEpoch 5/15\n----------------------------------------\n  Batch 0: Loss: 0.4435, Balanced: 80.2% (N:88.9%, A:71.4%)\n  Batch 20: Loss: 0.4373, Balanced: 82.6% (N:87.2%, A:78.0%)\n  Batch 40: Loss: 0.4097, Balanced: 82.6% (N:86.9%, A:78.4%)\n  Batch 60: Loss: 0.6698, Balanced: 82.3% (N:85.0%, A:79.5%)\n  Batch 80: Loss: 0.5382, Balanced: 81.4% (N:83.2%, A:79.6%)\n  Batch 100: Loss: 0.5340, Balanced: 80.9% (N:82.3%, A:79.4%)\n  Batch 120: Loss: 0.3849, Balanced: 80.4% (N:80.5%, A:80.4%)\nTrain - Loss: 0.5110, Acc: 80.3%, Balanced: 80.4%\n       Normal: 80.2%, Anomaly: 80.5%\nVal   - Loss: 0.5951, Acc: 71.7%, Balanced: 73.6%\n       Normal: 68.9%, Anomaly: 78.3%\n\nEpoch 6/15\n----------------------------------------\n  Batch 0: Loss: 0.4331, Balanced: 89.2% (N:95.0%, A:83.3%)\n  Batch 20: Loss: 0.3476, Balanced: 77.4% (N:72.1%, A:82.6%)\n  Batch 40: Loss: 0.6189, Balanced: 76.7% (N:72.2%, A:81.3%)\n  Batch 60: Loss: 0.5364, Balanced: 76.6% (N:71.6%, A:81.6%)\n  Batch 80: Loss: 0.4416, Balanced: 76.6% (N:71.5%, A:81.8%)\n  Batch 100: Loss: 0.6846, Balanced: 76.1% (N:70.3%, A:82.0%)\n  Batch 120: Loss: 0.6447, Balanced: 75.4% (N:69.6%, A:81.2%)\nTrain - Loss: 0.5684, Acc: 75.5%, Balanced: 75.6%\n       Normal: 69.8%, Anomaly: 81.3%\nVal   - Loss: 0.6219, Acc: 67.9%, Balanced: 71.0%\n       Normal: 63.3%, Anomaly: 78.7%\nâ° Early stopping after 5 epochs without improvement\n\nğŸ“Š FINAL EVALUATION\n==============================\nğŸ‰ WORKING SNN TRAINING COMPLETE!\nğŸ“Š Best Validation Balanced Accuracy: 83.6%\nğŸ“Š Test Results:\n   Overall Accuracy: 89.4%\n   Balanced Accuracy: 83.2%\n   Normal Accuracy: 98.6%\n   Anomaly Accuracy: 67.9%\n\nğŸ“‹ Classification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.88      0.99      0.93       560\n     Anomaly       0.95      0.68      0.79       240\n\n    accuracy                           0.89       800\n   macro avg       0.92      0.83      0.86       800\nweighted avg       0.90      0.89      0.89       800\n\n\nğŸ¯ WORKING SNN ANALYSIS:\nStability: âœ… STABLE (Both classes >30%)\nPerformance: âœ… GOOD (Balanced >83%)\nğŸ‰ SUCCESS: Working SNN ready for integration!\n\nğŸ¯ RECOMMENDATION: Integrate working SNN into hybrid CNN-LSTM-SNN pipeline\nğŸ“ Results saved to: working_snn_output\n\nâœ… Working SNN training completed!\n","output_type":"stream"}],"execution_count":7}]}